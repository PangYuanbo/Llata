import json
import os
from cerebras.cloud.sdk import Cerebras
from dotenv import load_dotenv

# 在程序入口处加载 .env 文件
load_dotenv()

# 然后你就可以用 os.environ.get 来获取变量
client = Cerebras(
    api_key=os.environ.get("CEREBRAS_API_KEY")
)

def llama_scoring_system(
    history_info,
    model_output,
    sympy_output,
    correct_solution,
    correct_answer,
    error_message=None
):
    """
    Interact with the AI LLAMA 3.1 -7B model for scoring and error hints.

    When the model fails to generate valid JSON, return:
    {
      "failed_generation": <the raw string generated by the model>
    }, along with an HTTP 400 status code.

    Notes:
      - JSON mode is incompatible with streaming; set stream to False.
      - When using JSON mode, explicitly instruct the model to generate JSON in the system or user message.
    """

    # Construct the content to be sent
    content = {
        "history_info": history_info,
        "model_output": model_output,
        "sympy_output": sympy_output,
        "correct_solution": correct_solution,
        "correct_answer": correct_answer,
        "error_message": error_message
    }

    # Clearly specify in the system message to return only JSON
    system_message = (
        "You are a scoring and error hint model, please return in JSON format only, "
        "the JSON should contain two fields:\n"
        "1. \"error_hint\": a string providing hints about errors or shortcomings\n"
        "2. \"score\": an integer representing the score (0-10)\n"
        "Please do not return anything other than JSON."
    )

    messages = [
        {"role": "system", "content": system_message},
        {"role": "user", "content": json.dumps(content, ensure_ascii=False)}
    ]

    # Call the scoring model, disable streaming output
    response = client.chat.completions.create(
        model="llama3.1-8b",  # The scoring model you are using
        messages=messages,
        stream=False         # Disable streaming output in JSON mode
    )

    # Get the model output
    model_raw_str = response["choices"][0]["message"]["content"]

    # Attempt to parse the output as JSON
    try:
        score_and_feedback = json.loads(model_raw_str)
    except json.JSONDecodeError:
        # If parsing fails, return a 400 error response with the raw string
        error_response = {
            "failed_generation": model_raw_str
        }
        # Assume returning (dict, 400) or throwing a custom exception
        return error_response, 400

    # If successfully parsed as JSON, ensure it contains error_hint and score fields
    if not isinstance(score_and_feedback, dict):
        # If not a dictionary structure, consider it a generation failure
        error_response = {
            "failed_generation": model_raw_str
        }
        return error_response, 400

    # If any fields are missing, consider it a generation failure
    if "error_hint" not in score_and_feedback or "score" not in score_and_feedback:
        error_response = {
            "failed_generation": model_raw_str
        }
        return error_response, 400

    # If everything is fine, return normally
    return score_and_feedback, 200
