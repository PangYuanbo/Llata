{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": "!pip install -q transformers[torch] datasets"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T23:28:38.769034Z",
     "start_time": "2025-01-19T23:28:14.864266Z"
    }
   },
   "cell_type": "code",
   "source": "!pip install -q bitsandbytes trl peft",
   "id": "b3d57d6dd9bf7ecd",
   "outputs": [],
   "execution_count": 20
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "!pip install flash-attn --no-build-isolation",
   "id": "9b70215a5a08f32b"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:28:39.532437Z",
     "start_time": "2025-01-20T00:28:37.527126Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import io\n",
    "\n",
    "# 使用 Windows-1252 编码读取原始文件\n",
    "with io.open(\"training_dataset.jsonl\", \"r\", encoding=\"Windows-1252\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "# 以 UTF-8 编码写入新文件\n",
    "with io.open(\"training_dataset_utf8.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(data)\n",
    "\n",
    "# 使用转换后的文件加载数据集\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"training_dataset_utf8.jsonl\")\n",
    "print(dataset)\n"
   ],
   "id": "d9749002986088ca",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\anaconda3\\envs\\Llata\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 967 examples [00:00, 161165.54 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 967\n",
      "    })\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:41:32.303468Z",
     "start_time": "2025-01-20T00:41:32.295469Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example = dataset[\"train\"][3]\n",
    "print(example[\"input\"])"
   ],
   "id": "55c362470513fb21",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: You are a math solving assistant and need to use SymPy to help with calculations. Follow these step-by-step instructions using only symbols and expression (expr) operations:\n",
      "Step 1: Output only the symbol 'x'.\n",
      "Step 2: Output a mathematical expression involving 'x' (e.g., x**2 + 2*x + 1). Do not include variable assignment; only provide the expression itself.\n",
      "Step 3: Output only the name of a SymPy function you intend to use on the expression (e.g., expand, solve, Eq).\n",
      "Step 4: Output the arguments you would pass to that function, separated by commas (e.g., expr, x).\n",
      "\n",
      "step 5: Wheather you need to continue the next step(e.g., expr,x).\n",
      "\n",
      "Your final output should be a single JSON block like the example below:\n",
      "\"```json\n",
      "        {\n",
      "            \"symbol\": \"x\",\n",
      "            \"expr\": \"x+8\",\n",
      "            \"sympy_function\": \"solve\",\n",
      "            \"function_args\": \"expr, x\"\n",
      "            \"require_next_step\": \"True\"\n",
      "        }\n",
      "\n",
      "        ```\n",
      "        \"Remember you don't need to do any additional calculation, just provide the json.\"\n",
      "         \"Do not output anything else.\"\n",
      "        \n",
      "Question: James has 20 pairs of red socks and half as many black socks.  He has twice as many white socks as red and black combined.  How many total socks does he have combined?\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:28:40.180273Z",
     "start_time": "2025-01-20T00:28:39.774675Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 使用 chardet 库检测文件编码\n",
    "import chardet\n",
    "\n",
    "with open(\"training_dataset.jsonl\", \"rb\") as f:\n",
    "    result = chardet.detect(f.read())\n",
    "print(result)  # 打印检测到的编码信息\n"
   ],
   "id": "ac17535f29cc69e8",
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chardet'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mModuleNotFoundError\u001B[0m                       Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[2], line 2\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;66;03m# 使用 chardet 库检测文件编码\u001B[39;00m\n\u001B[1;32m----> 2\u001B[0m \u001B[38;5;28;01mimport\u001B[39;00m \u001B[38;5;21;01mchardet\u001B[39;00m\n\u001B[0;32m      4\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m \u001B[38;5;28mopen\u001B[39m(\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mtraining_dataset.jsonl\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mrb\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;28;01mas\u001B[39;00m f:\n\u001B[0;32m      5\u001B[0m     result \u001B[38;5;241m=\u001B[39m chardet\u001B[38;5;241m.\u001B[39mdetect(f\u001B[38;5;241m.\u001B[39mread())\n",
      "\u001B[1;31mModuleNotFoundError\u001B[0m: No module named 'chardet'"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:28:43.635013Z",
     "start_time": "2025-01-20T00:28:43.578005Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# 假设你已经导入并加载了 dataset 以及 messages_to_string 函数\n",
    "# 将 messages 拼接为单个字符串\n",
    "def messages_to_string(messages):\n",
    "    \"\"\"\n",
    "    将 messages 转换为单个字符串，保留对话上下文。\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "    for message in messages:\n",
    "        # 拼接角色和内容\n",
    "        context += f\"{message['role']}: {message['content']}\\n\"\n",
    "    return context.strip()  # 移除最后一个多余的换行符\n",
    "def process_example(example):\n",
    "    # 取出原始 input 内容\n",
    "    question_prompt = example[\"input\"]\n",
    "    # 使用 messages_to_string 函数处理\n",
    "    messages = messages_to_string([\n",
    "        {\"role\": \"user\", \"content\": question_prompt}\n",
    "    ])\n",
    "    # 更新 input 字段\n",
    "    example[\"input\"] = messages\n",
    "    return example\n",
    "\n",
    "# 对 train 分割区的每个样本应用处理函数\n",
    "dataset[\"train\"] = dataset[\"train\"].map(process_example)\n"
   ],
   "id": "d3fee18597bf56b2",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 967/967 [00:00<00:00, 26862.52 examples/s]\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:28:46.073171Z",
     "start_time": "2025-01-20T00:28:46.055170Z"
    }
   },
   "cell_type": "code",
   "source": [
    "example = dataset[\"train\"][3]\n",
    "print(example[\"input\"])"
   ],
   "id": "1e73c7a2fb58ffa4",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: You are a math solving assistant and need to use SymPy to help with calculations. Follow these step-by-step instructions using only symbols and expression (expr) operations:\n",
      "Step 1: Output only the symbol 'x'.\n",
      "Step 2: Output a mathematical expression involving 'x' (e.g., x**2 + 2*x + 1). Do not include variable assignment; only provide the expression itself.\n",
      "Step 3: Output only the name of a SymPy function you intend to use on the expression (e.g., expand, solve, Eq).\n",
      "Step 4: Output the arguments you would pass to that function, separated by commas (e.g., expr, x).\n",
      "\n",
      "step 5: Wheather you need to continue the next step(e.g., expr,x).\n",
      "\n",
      "Your final output should be a single JSON block like the example below:\n",
      "\"```json\n",
      "        {\n",
      "            \"symbol\": \"x\",\n",
      "            \"expr\": \"x+8\",\n",
      "            \"sympy_function\": \"solve\",\n",
      "            \"function_args\": \"expr, x\"\n",
      "            \"require_next_step\": \"True\"\n",
      "        }\n",
      "\n",
      "        ```\n",
      "        \"Remember you don't need to do any additional calculation, just provide the json.\"\n",
      "         \"Do not output anything else.\"\n",
      "        \n",
      "Question: James has 20 pairs of red socks and half as many black socks.  He has twice as many white socks as red and black combined.  How many total socks does he have combined?\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:29:03.259113Z",
     "start_time": "2025-01-20T00:29:01.486903Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=False)\n",
    "# model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=False)\n",
    "\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# # Set reasonable default for models without max length\n",
    "# if tokenizer.model_max_length > 100_000:\n",
    "#   tokenizer.model_max_length = 2048\n",
    "\n",
    "# # Set chat template\n",
    "# DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "# tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ],
   "id": "52efa85ac05341e8",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aaron\\anaconda3\\envs\\Llata\\lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:809: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "4183f99f511cfd8a"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:30:42.848106Z",
     "start_time": "2025-01-20T00:30:42.837104Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# specify how to quantize the model\n",
    "# quantization_config = BitsAndBytesConfig(\n",
    "#             load_in_4bit=True,\n",
    "#             bnb_4bit_quant_type=\"nf4\",\n",
    "#             bnb_4bit_compute_dtype=\"torch.bfloat16\",\n",
    "# )\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "\n",
    "model_kwargs = dict(\n",
    "    # attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "    torch_dtype=\"auto\",\n",
    "    use_cache=False, # set to False as we're going to use gradient checkpointing\n",
    "    device_map=device_map,\n",
    "    # quantization_config=quantization_config,\n",
    ")"
   ],
   "id": "593d29208967bdbe",
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "51f2810c07726e06"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "572fe223cce5cd3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:55:25.530118Z",
     "start_time": "2025-01-20T00:55:23.802820Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "def formatting_prompts_func(example):\n",
    "    # 确保 example 是一个包含 \"input\" 和 \"output\" 键的字典\n",
    "    input_text = example[\"input\"]\n",
    "    output_text = example[\"output\"]\n",
    "    \n",
    "    # 拼接为指定格式的文本\n",
    "    text = f\"### Question: {input_text}\\n### Answer: {output_text}\"\n",
    "    \n",
    "    # 可选：打印输出以调试\n",
    "    print(text)\n",
    "    \n",
    "    return text\n",
    "\n",
    "# path where the Trainer will save its checkpoints and logs\n",
    "output_dir = 'data/zephyr-7b-sft-lora'\n",
    "\n",
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    # fp16=True, # specify bf16=True instead when training on GPUs that support bf16\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"no\",\n",
    "    gradient_accumulation_steps=128,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2.0e-05,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=4, # originally set to 8\n",
    "    per_device_train_batch_size=4, # originally set to 8\n",
    "    # push_to_hub=True,\n",
    "    # hub_model_id=\"zephyr-7b-sft-lora\",\n",
    "    # hub_strategy=\"every_save\",\n",
    "    # report_to=\"tensorboard\",\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=None,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model_name,\n",
    "        model_init_kwargs=model_kwargs,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        tokenizer=tokenizer,\n",
    "        packing=True,\n",
    "        peft_config=peft_config,\n",
    "        formatting_func=formatting_prompts_func,\n",
    "        max_seq_length=tokenizer.model_max_length,\n",
    "    )"
   ],
   "id": "27bbef5c3cb15da",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:00<00:00,  4.39it/s]\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-01-20T00:56:02.506090Z"
    }
   },
   "cell_type": "code",
   "source": "train_result = trainer.train()",
   "id": "e117d876c5fa6f94",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 2\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 128\n",
      "  Total optimization steps = 1\n",
      "  Number of trainable parameters = 36,700,160\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "metrics = train_result.metrics\n",
    "max_train_samples = training_args.max_train_samples if training_args.max_train_samples is not None else len(train_dataset)\n",
    "metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()\n",
    "trainer.save_model(output_dir)"
   ],
   "id": "80e77c2841a23d7d"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
