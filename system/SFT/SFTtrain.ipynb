{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers[torch] datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d57d6dd9bf7ecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T23:28:38.769034Z",
     "start_time": "2025-01-19T23:28:14.864266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q bitsandbytes trl peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b70215a5a08f32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.7.3.tar.gz (3.2 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.2.1)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.7.3-cp310-cp310-linux_x86_64.whl size=191240169 sha256=944caed3c7c96721d8a35d44a7821d8a9a7c73c353ecc85bcf6bf44f6a2559c0\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/d7/10/a74c9fe5ffe6ff306b27a220b2bf2f37d907b68fdcd138cdda\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.0 flash-attn-2.7.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9749002986088ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:28:39.532437Z",
     "start_time": "2025-01-20T00:28:37.527126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa9351d3ab6449fae20fb4204279486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 1561\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "# 使用 Windows-1252 编码读取原始文件\n",
    "with io.open(\"training_dataset.jsonl\", \"r\", encoding=\"Windows-1252\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "# 以 UTF-8 编码写入新文件\n",
    "with io.open(\"training_dataset_utf8.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(data)\n",
    "\n",
    "# 使用转换后的文件加载数据集\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"training_dataset_utf8.jsonl\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55c362470513fb21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:41:32.303468Z",
     "start_time": "2025-01-20T00:41:32.295469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: You are a math solving assistant and need to use SymPy to help with calculations. Follow these step-by-step instructions using only symbols and expression (expr) operations:\n",
      "Step 1: Output only the symbol 'x'.\n",
      "Step 2: Output a mathematical expression involving 'x' (e.g., x**2 + 2*x + 1). Do not include variable assignment; only provide the expression itself.\n",
      "Step 3: Output only the name of a SymPy function you intend to use on the expression (e.g., expand, solve, Eq).\n",
      "Step 4: Output the arguments you would pass to that function, separated by commas (e.g., expr, x).\n",
      "\n",
      "step 5: Wheather you need to continue the next step(e.g., expr,x).\n",
      "\n",
      "Your final output should be a single JSON block like the example below:\n",
      "\"```json\n",
      "        {\n",
      "            \"symbol\": \"x\",\n",
      "            \"expr\": \"x+8\",\n",
      "            \"sympy_function\": \"solve\",\n",
      "            \"function_args\": \"expr, x\"\n",
      "            \"require_next_step\": \"True\"\n",
      "        }\n",
      "\n",
      "        ```\n",
      "        \"Remember you don't need to do any additional calculation, just provide the json.\"\n",
      "         \"Do not output anything else.\"\n",
      "        \n",
      "Question: James has 20 pairs of red socks and half as many black socks.  He has twice as many white socks as red and black combined.  How many total socks does he have combined?\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][3]\n",
    "print(example[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac17535f29cc69e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:28:40.180273Z",
     "start_time": "2025-01-20T00:28:39.774675Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chardet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# 使用 chardet 库检测文件编码\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_dataset.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     result \u001b[38;5;241m=\u001b[39m chardet\u001b[38;5;241m.\u001b[39mdetect(f\u001b[38;5;241m.\u001b[39mread())\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chardet'"
     ]
    }
   ],
   "source": [
    "# 使用 chardet 库检测文件编码\n",
    "import chardet\n",
    "\n",
    "with open(\"training_dataset.jsonl\", \"rb\") as f:\n",
    "    result = chardet.detect(f.read())\n",
    "print(result)  # 打印检测到的编码信息\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3fee18597bf56b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:28:43.635013Z",
     "start_time": "2025-01-20T00:28:43.578005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbdecd19c1f48958ea54a43afc8d538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1561 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 假设你已经导入并加载了 dataset 以及 messages_to_string 函数\n",
    "# 将 messages 拼接为单个字符串\n",
    "def messages_to_string(messages):\n",
    "    \"\"\"\n",
    "    将 messages 转换为单个字符串，保留对话上下文。\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "    for message in messages:\n",
    "        # 拼接角色和内容\n",
    "        context += f\"{message['role']}: {message['content']}\\n\"\n",
    "    return context.strip()  # 移除最后一个多余的换行符\n",
    "def process_example(example):\n",
    "    # 取出原始 input 内容\n",
    "    question_prompt = example[\"input\"]\n",
    "    # 使用 messages_to_string 函数处理\n",
    "    messages = messages_to_string([\n",
    "        {\"role\": \"user\", \"content\": question_prompt}\n",
    "    ])\n",
    "    # 更新 input 字段\n",
    "    example[\"input\"] = messages\n",
    "    return example\n",
    "\n",
    "# 对 train 分割区的每个样本应用处理函数\n",
    "dataset[\"train\"] = dataset[\"train\"].map(process_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e73c7a2fb58ffa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:28:46.073171Z",
     "start_time": "2025-01-20T00:28:46.055170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: You are a math solving assistant and need to use SymPy to help with calculations. Follow these step-by-step instructions using only symbols and expression (expr) operations:\n",
      "Step 1: Output only the symbol 'x'.\n",
      "Step 2: Output a mathematical expression involving 'x' (e.g., x**2 + 2*x + 1). Do not include variable assignment; only provide the expression itself.\n",
      "Step 3: Output only the name of a SymPy function you intend to use on the expression (e.g., expand, solve, Eq).\n",
      "Step 4: Output the arguments you would pass to that function, separated by commas (e.g., expr, x).\n",
      "\n",
      "step 5: Wheather you need to continue the next step(e.g., expr,x).\n",
      "\n",
      "Your final output should be a single JSON block like the example below:\n",
      "\"```json\n",
      "        {\n",
      "            \"symbol\": \"x\",\n",
      "            \"expr\": \"x+8\",\n",
      "            \"sympy_function\": \"solve\",\n",
      "            \"function_args\": \"expr, x\"\n",
      "            \"require_next_step\": \"True\"\n",
      "        }\n",
      "\n",
      "        ```\n",
      "        \"Remember you don't need to do any additional calculation, just provide the json.\"\n",
      "         \"Do not output anything else.\"\n",
      "        \n",
      "Question: James has 20 pairs of red socks and half as many black socks.  He has twice as many white socks as red and black combined.  How many total socks does he have combined?\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][3]\n",
    "print(example[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52efa85ac05341e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:29:03.259113Z",
     "start_time": "2025-01-20T00:29:01.486903Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "\n",
    "\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# # Set reasonable default for models without max length\n",
    "# if tokenizer.model_max_length > 100_000:\n",
    "#   tokenizer.model_max_length = 2048\n",
    "\n",
    "# # Set chat template\n",
    "# DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "# tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183f99f511cfd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "593d29208967bdbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:30:42.848106Z",
     "start_time": "2025-01-20T00:30:42.837104Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19f502022ce44ef870e22f2259a50e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True,attn_implementation=\"flash_attention_2\",quantization_config=quantization_config)\n",
    "# model_kwargs = dict(\n",
    "#     attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "#     torch_dtype=\"auto\",\n",
    "#     use_cache=False, # set to False as we're going to use gradient checkpointing\n",
    "#     device_map=device_map,\n",
    "#     # quantization_config=quantization_config,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f2810c07726e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "572fe223cce5cd3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27bbef5c3cb15da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:55:25.530118Z",
     "start_time": "2025-01-20T00:55:23.802820Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_127/2551703161.py:52: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f791eb5c979b443090aaa0668dc962d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1561 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['input'])):\n",
    "        text = f\"### Question: {example['input'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "# path where the Trainer will save its checkpoints and logs\n",
    "output_dir = 'data/zephyr-7b-sft-lora'\n",
    "\n",
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    # fp16=True, # specify bf16=True instead when training on GPUs that support bf16\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"no\",\n",
    "    gradient_accumulation_steps=128,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2.0e-05,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=4, # originally set to 8\n",
    "    per_device_train_batch_size=4, # originally set to 8\n",
    "    # push_to_hub=True,\n",
    "    # hub_model_id=\"zephyr-7b-sft-lora\",\n",
    "    # hub_strategy=\"every_save\",\n",
    "    # report_to=\"tensorboard\",\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=None,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        tokenizer=tokenizer,\n",
    "        peft_config=peft_config,\n",
    "        formatting_func=formatting_prompts_func\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e117d876c5fa6f94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T02:59:32.819469Z",
     "start_time": "2025-01-20T00:56:02.506090Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1,561\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 128\n",
      "  Total optimization steps = 3\n",
      "  Number of trainable parameters = 36,700,160\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 07:48, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80e77c2841a23d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to data/zephyr-7b-sft-lora\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in data/zephyr-7b-sft-lora/tokenizer_config.json\n",
      "Special tokens file saved in data/zephyr-7b-sft-lora/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# metrics = train_result.metrics\n",
    "# max_train_samples = training_args.max_train_samples if training_args.max_train_samples is not None else len(train_dataset)\n",
    "# metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "# trainer.log_metrics(\"train\", metrics)\n",
    "# trainer.save_metrics(\"train\", metrics)\n",
    "# trainer.save_state()\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52599950-0b19-4df2-b434-20451f5adec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
