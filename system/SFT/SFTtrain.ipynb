{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "initial_id",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q transformers[torch] datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d57d6dd9bf7ecd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-19T23:28:38.769034Z",
     "start_time": "2025-01-19T23:28:14.864266Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install -q bitsandbytes trl peft"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9b70215a5a08f32b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting flash-attn\n",
      "  Downloading flash_attn-2.7.3.tar.gz (3.2 MB)\n",
      "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hRequirement already satisfied: torch in /opt/conda/lib/python3.10/site-packages (from flash-attn) (2.2.1)\n",
      "Collecting einops (from flash-attn)\n",
      "  Downloading einops-0.8.0-py3-none-any.whl.metadata (12 kB)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (4.12.2)\n",
      "Requirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (1.12)\n",
      "Requirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1)\n",
      "Requirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (3.1.3)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.10/site-packages (from torch->flash-attn) (2024.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch->flash-attn) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in /opt/conda/lib/python3.10/site-packages (from sympy->torch->flash-attn) (1.3.0)\n",
      "Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: flash-attn\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for flash-attn: filename=flash_attn-2.7.3-cp310-cp310-linux_x86_64.whl size=191240169 sha256=944caed3c7c96721d8a35d44a7821d8a9a7c73c353ecc85bcf6bf44f6a2559c0\n",
      "  Stored in directory: /root/.cache/pip/wheels/85/d7/10/a74c9fe5ffe6ff306b27a220b2bf2f37d907b68fdcd138cdda\n",
      "Successfully built flash-attn\n",
      "Installing collected packages: einops, flash-attn\n",
      "Successfully installed einops-0.8.0 flash-attn-2.7.3\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install flash-attn --no-build-isolation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9749002986088ca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:28:39.532437Z",
     "start_time": "2025-01-20T00:28:37.527126Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa9351d3ab6449fae20fb4204279486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['input', 'output'],\n",
      "        num_rows: 1561\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "import io\n",
    "\n",
    "# ‰ΩøÁî® Windows-1252 ÁºñÁ†ÅËØªÂèñÂéüÂßãÊñá‰ª∂\n",
    "with io.open(\"training_dataset.jsonl\", \"r\", encoding=\"Windows-1252\") as f:\n",
    "    data = f.read()\n",
    "\n",
    "# ‰ª• UTF-8 ÁºñÁ†ÅÂÜôÂÖ•Êñ∞Êñá‰ª∂\n",
    "with io.open(\"training_dataset_utf8.jsonl\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(data)\n",
    "\n",
    "# ‰ΩøÁî®ËΩ¨Êç¢ÂêéÁöÑÊñá‰ª∂Âä†ËΩΩÊï∞ÊçÆÈõÜ\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"json\", data_files=\"training_dataset_utf8.jsonl\")\n",
    "print(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "55c362470513fb21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:41:32.303468Z",
     "start_time": "2025-01-20T00:41:32.295469Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: You are a math solving assistant and need to use SymPy to help with calculations. Follow these step-by-step instructions using only symbols and expression (expr) operations:\n",
      "Step 1: Output only the symbol 'x'.\n",
      "Step 2: Output a mathematical expression involving 'x' (e.g., x**2 + 2*x + 1). Do not include variable assignment; only provide the expression itself.\n",
      "Step 3: Output only the name of a SymPy function you intend to use on the expression (e.g., expand, solve, Eq).\n",
      "Step 4: Output the arguments you would pass to that function, separated by commas (e.g., expr, x).\n",
      "\n",
      "step 5: Wheather you need to continue the next step(e.g., expr,x).\n",
      "\n",
      "Your final output should be a single JSON block like the example below:\n",
      "\"```json\n",
      "        {\n",
      "            \"symbol\": \"x\",\n",
      "            \"expr\": \"x+8\",\n",
      "            \"sympy_function\": \"solve\",\n",
      "            \"function_args\": \"expr, x\"\n",
      "            \"require_next_step\": \"True\"\n",
      "        }\n",
      "\n",
      "        ```\n",
      "        \"Remember you don't need to do any additional calculation, just provide the json.\"\n",
      "         \"Do not output anything else.\"\n",
      "        \n",
      "Question: James has 20 pairs of red socks and half as many black socks.  He has twice as many white socks as red and black combined.  How many total socks does he have combined?\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][3]\n",
    "print(example[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ac17535f29cc69e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:28:40.180273Z",
     "start_time": "2025-01-20T00:28:39.774675Z"
    }
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'chardet'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# ‰ΩøÁî® chardet Â∫ìÊ£ÄÊµãÊñá‰ª∂ÁºñÁ†Å\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mchardet\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtraining_dataset.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrb\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m      5\u001b[0m     result \u001b[38;5;241m=\u001b[39m chardet\u001b[38;5;241m.\u001b[39mdetect(f\u001b[38;5;241m.\u001b[39mread())\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'chardet'"
     ]
    }
   ],
   "source": [
    "# ‰ΩøÁî® chardet Â∫ìÊ£ÄÊµãÊñá‰ª∂ÁºñÁ†Å\n",
    "import chardet\n",
    "\n",
    "with open(\"training_dataset.jsonl\", \"rb\") as f:\n",
    "    result = chardet.detect(f.read())\n",
    "print(result)  # ÊâìÂç∞Ê£ÄÊµãÂà∞ÁöÑÁºñÁ†Å‰ø°ÊÅØ\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d3fee18597bf56b2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:28:43.635013Z",
     "start_time": "2025-01-20T00:28:43.578005Z"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbbdecd19c1f48958ea54a43afc8d538",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1561 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# ÂÅáËÆæ‰Ω†Â∑≤ÁªèÂØºÂÖ•Âπ∂Âä†ËΩΩ‰∫Ü dataset ‰ª•Âèä messages_to_string ÂáΩÊï∞\n",
    "# Â∞Ü messages ÊãºÊé•‰∏∫Âçï‰∏™Â≠óÁ¨¶‰∏≤\n",
    "def messages_to_string(messages):\n",
    "    \"\"\"\n",
    "    Â∞Ü messages ËΩ¨Êç¢‰∏∫Âçï‰∏™Â≠óÁ¨¶‰∏≤Ôºå‰øùÁïôÂØπËØù‰∏ä‰∏ãÊñá„ÄÇ\n",
    "    \"\"\"\n",
    "    context = \"\"\n",
    "    for message in messages:\n",
    "        # ÊãºÊé•ËßíËâ≤ÂíåÂÜÖÂÆπ\n",
    "        context += f\"{message['role']}: {message['content']}\\n\"\n",
    "    return context.strip()  # ÁßªÈô§ÊúÄÂêé‰∏Ä‰∏™Â§ö‰ΩôÁöÑÊç¢Ë°åÁ¨¶\n",
    "def process_example(example):\n",
    "    # ÂèñÂá∫ÂéüÂßã input ÂÜÖÂÆπ\n",
    "    question_prompt = example[\"input\"]\n",
    "    # ‰ΩøÁî® messages_to_string ÂáΩÊï∞Â§ÑÁêÜ\n",
    "    messages = messages_to_string([\n",
    "        {\"role\": \"user\", \"content\": question_prompt}\n",
    "    ])\n",
    "    # Êõ¥Êñ∞ input Â≠óÊÆµ\n",
    "    example[\"input\"] = messages\n",
    "    return example\n",
    "\n",
    "# ÂØπ train ÂàÜÂâ≤Âå∫ÁöÑÊØè‰∏™Ê†∑Êú¨Â∫îÁî®Â§ÑÁêÜÂáΩÊï∞\n",
    "dataset[\"train\"] = dataset[\"train\"].map(process_example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1e73c7a2fb58ffa4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:28:46.073171Z",
     "start_time": "2025-01-20T00:28:46.055170Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: You are a math solving assistant and need to use SymPy to help with calculations. Follow these step-by-step instructions using only symbols and expression (expr) operations:\n",
      "Step 1: Output only the symbol 'x'.\n",
      "Step 2: Output a mathematical expression involving 'x' (e.g., x**2 + 2*x + 1). Do not include variable assignment; only provide the expression itself.\n",
      "Step 3: Output only the name of a SymPy function you intend to use on the expression (e.g., expand, solve, Eq).\n",
      "Step 4: Output the arguments you would pass to that function, separated by commas (e.g., expr, x).\n",
      "\n",
      "step 5: Wheather you need to continue the next step(e.g., expr,x).\n",
      "\n",
      "Your final output should be a single JSON block like the example below:\n",
      "\"```json\n",
      "        {\n",
      "            \"symbol\": \"x\",\n",
      "            \"expr\": \"x+8\",\n",
      "            \"sympy_function\": \"solve\",\n",
      "            \"function_args\": \"expr, x\"\n",
      "            \"require_next_step\": \"True\"\n",
      "        }\n",
      "\n",
      "        ```\n",
      "        \"Remember you don't need to do any additional calculation, just provide the json.\"\n",
      "         \"Do not output anything else.\"\n",
      "        \n",
      "Question: James has 20 pairs of red socks and half as many black socks.  He has twice as many white socks as red and black combined.  How many total socks does he have combined?\n"
     ]
    }
   ],
   "source": [
    "example = dataset[\"train\"][3]\n",
    "print(example[\"input\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "52efa85ac05341e8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:29:03.259113Z",
     "start_time": "2025-01-20T00:29:01.486903Z"
    }
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-3.2-3B-Instruct\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_auth_token=True)\n",
    "\n",
    "\n",
    "\n",
    "# set pad_token_id equal to the eos_token_id if not set\n",
    "if tokenizer.pad_token_id is None:\n",
    "  tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "\n",
    "# # Set reasonable default for models without max length\n",
    "# if tokenizer.model_max_length > 100_000:\n",
    "#   tokenizer.model_max_length = 2048\n",
    "\n",
    "# # Set chat template\n",
    "# DEFAULT_CHAT_TEMPLATE = \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\"\n",
    "# tokenizer.chat_template = DEFAULT_CHAT_TEMPLATE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4183f99f511cfd8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "593d29208967bdbe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:30:42.848106Z",
     "start_time": "2025-01-20T00:30:42.837104Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19f502022ce44ef870e22f2259a50e7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import BitsAndBytesConfig\n",
    "import torch\n",
    "\n",
    "# specify how to quantize the model\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "            load_in_4bit=True,\n",
    "            bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "device_map = {\"\": torch.cuda.current_device()} if torch.cuda.is_available() else None\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, use_auth_token=True,attn_implementation=\"flash_attention_2\",quantization_config=quantization_config)\n",
    "# model_kwargs = dict(\n",
    "#     attn_implementation=\"flash_attention_2\", # set this to True if your GPU supports it (Flash Attention drastically speeds up model computations)\n",
    "#     torch_dtype=\"auto\",\n",
    "#     use_cache=False, # set to False as we're going to use gradient checkpointing\n",
    "#     device_map=device_map,\n",
    "#     # quantization_config=quantization_config,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51f2810c07726e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "572fe223cce5cd3d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "27bbef5c3cb15da",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T00:55:25.530118Z",
     "start_time": "2025-01-20T00:55:23.802820Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_127/2551703161.py:52: FutureWarning: `tokenizer` is deprecated and removed starting from version 0.16.0 for `SFTTrainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = SFTTrainer(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "/opt/conda/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f791eb5c979b443090aaa0668dc962d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1561 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import SFTTrainer\n",
    "from peft import LoraConfig\n",
    "from transformers import TrainingArguments\n",
    "def formatting_prompts_func(example):\n",
    "    output_texts = []\n",
    "    for i in range(len(example['input'])):\n",
    "        text = f\"### Question: {example['input'][i]}\\n ### Answer: {example['output'][i]}\"\n",
    "        output_texts.append(text)\n",
    "    return output_texts\n",
    "\n",
    "# path where the Trainer will save its checkpoints and logs\n",
    "output_dir = 'data/zephyr-7b-sft-lora'\n",
    "\n",
    "# based on config\n",
    "training_args = TrainingArguments(\n",
    "    # fp16=True, # specify bf16=True instead when training on GPUs that support bf16\n",
    "    do_eval=True,\n",
    "    evaluation_strategy=\"no\",\n",
    "    gradient_accumulation_steps=128,\n",
    "    gradient_checkpointing=True,\n",
    "    gradient_checkpointing_kwargs={\"use_reentrant\": False},\n",
    "    learning_rate=2.0e-05,\n",
    "    log_level=\"info\",\n",
    "    logging_steps=5,\n",
    "    logging_strategy=\"steps\",\n",
    "    lr_scheduler_type=\"cosine\",\n",
    "    max_steps=-1,\n",
    "    num_train_epochs=1,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    per_device_eval_batch_size=4, # originally set to 8\n",
    "    per_device_train_batch_size=4, # originally set to 8\n",
    "    # push_to_hub=True,\n",
    "    # hub_model_id=\"zephyr-7b-sft-lora\",\n",
    "    # hub_strategy=\"every_save\",\n",
    "    # report_to=\"tensorboard\",\n",
    "    save_strategy=\"no\",\n",
    "    save_total_limit=None,\n",
    "    seed=42,\n",
    ")\n",
    "\n",
    "# based on config\n",
    "peft_config = LoraConfig(\n",
    "        r=64,\n",
    "        lora_alpha=16,\n",
    "        lora_dropout=0.1,\n",
    "        bias=\"none\",\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"],\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset[\"train\"],\n",
    "        tokenizer=tokenizer,\n",
    "        peft_config=peft_config,\n",
    "        formatting_func=formatting_prompts_func\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e117d876c5fa6f94",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-20T02:59:32.819469Z",
     "start_time": "2025-01-20T00:56:02.506090Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 1,561\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 4\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 512\n",
      "  Gradient Accumulation steps = 128\n",
      "  Total optimization steps = 3\n",
      "  Number of trainable parameters = 36,700,160\n",
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n",
      "The input hidden states seems to be silently casted in float32, this might be related to the fact you have upcasted embedding or layer norm layers in float32. We will cast back the input in torch.float16.\n",
      "/opt/conda/lib/python3.10/site-packages/bitsandbytes/nn/modules.py:451: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 07:48, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train_result = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "80e77c2841a23d7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to data/zephyr-7b-sft-lora\n",
      "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Llama-3.2-3B-Instruct/snapshots/0cb88a4f764b7a12671c53f0838cd831a0843b95/config.json\n",
      "Model config LlamaConfig {\n",
      "  \"architectures\": [\n",
      "    \"LlamaForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 128000,\n",
      "  \"eos_token_id\": [\n",
      "    128001,\n",
      "    128008,\n",
      "    128009\n",
      "  ],\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"mlp_bias\": false,\n",
      "  \"model_type\": \"llama\",\n",
      "  \"num_attention_heads\": 24,\n",
      "  \"num_hidden_layers\": 28,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pretraining_tp\": 1,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"factor\": 32.0,\n",
      "    \"high_freq_factor\": 4.0,\n",
      "    \"low_freq_factor\": 1.0,\n",
      "    \"original_max_position_embeddings\": 8192,\n",
      "    \"rope_type\": \"llama3\"\n",
      "  },\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"tie_word_embeddings\": true,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.48.1\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n",
      "\n",
      "tokenizer config file saved in data/zephyr-7b-sft-lora/tokenizer_config.json\n",
      "Special tokens file saved in data/zephyr-7b-sft-lora/special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# metrics = train_result.metrics\n",
    "# max_train_samples = training_args.max_train_samples if training_args.max_train_samples is not None else len(train_dataset)\n",
    "# metrics[\"train_samples\"] = min(max_train_samples, len(train_dataset))\n",
    "# trainer.log_metrics(\"train\", metrics)\n",
    "# trainer.save_metrics(\"train\", metrics)\n",
    "# trainer.save_state()\n",
    "trainer.save_model(output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52599950-0b19-4df2-b434-20451f5adec7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
